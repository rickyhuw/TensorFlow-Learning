{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer\n",
    "## 1.梯度下降法（Gradient Descent）\n",
    "* 标准的梯度下降法\n",
    "    - 先计算所有样本的总误差，然后根据总误差更新权值\n",
    "* 随机梯度下降法\n",
    "    - 随机抽取一个样本计算误差，然后更新权值\n",
    "* 批量梯度下降法\n",
    "    - 随机抽取一个batch的样本计算误差，然后更新权值\n",
    "* 公式：$ W = W - \\eta \\bullet \\nabla J(W;x^{(i)},y^{(i)})$\n",
    "   \n",
    "## 2.动力下降法（Momentum）\n",
    "* 当前权值的改变会受到上一次改变的影响。比如小球下坡的时候会带有惯性，因此在上一次的梯度较大时，下一次会加速下降。\n",
    "* $\\gamma$:动力，一般取值为0.9\n",
    "* $V_t = \\gamma V_{t-1} + \\eta \\bullet \\nabla J(W)$\n",
    "* $W = W - V_t$\n",
    "\n",
    "## 3.NAG（Nesterov accelerated gradient）\n",
    "* 内斯特罗夫加速梯度\n",
    "* 这个是对“动力下降法”的改良，因为动力下降，在接近谷底的时候，会因为惯性的原因，导致跳出最优解，因此NAG在此基础之上，添加了对小球下一次位置的预测，然后应用到本次的下降中。\n",
    "* $V_t = \\gamma V_{t-1} + \\eta \\bullet \\nabla J(W - \\gamma V_{t-1})$\n",
    "* $W = W - V_t$\n",
    "\n",
    "## 4.Adagrad\n",
    "* $i$:表示第i个分类\n",
    "* $t$:表示某个类别出现的次数\n",
    "* $\\epsilon$:表示一个非常小的值，防止除0\n",
    "* $\\eta$:学习率\n",
    "* $g_{t,i} =  \\nabla _w J(W)$: i类别在第t次的梯度\n",
    "* $W_{t + 1} = W_t - \\frac {\\eta}{\\sqrt{\\sum_{t^\"=1}^{t}{(g_{t^\",i})}^2 + \\epsilon }} \\bullet g_t$\n",
    "* 它是基于SGD思想的一种优化算法，对于比较常见的数据（分类）会有比较小的学习率，对于比较罕见的数据（分类）会有比较大的学习率，使用与比较稀疏的数据集。因此他的优点也恰恰是他的缺点，优点是可以自动的调整学习率；缺点是随着迭代次数的增大，学习率会越来越小，最终趋于0.\n",
    "\n",
    "\n",
    "## 5.RMSprop\n",
    "* 和Adagrad的思想非常相似，唯一的不同是，不在是记录所有的t了，而是取前t次的累加。\n",
    "\n",
    "## 6.Adadelta\n",
    "\n",
    "## 7. Adam\n",
    "* $\\beta _1$:一般取值为0.9\n",
    "* $\\beta _2$:一般取值为0.99\n",
    "* $\\epsilon$:表示一个非常小的值，防止除0\n",
    "* $m_t = \\beta _1 m_{t-1} + (1-\\beta _1)g_t$\n",
    "* $v_t = \\beta _2 v_{t-1} + (1- \\beta _2)g_t^2$\n",
    "* $\\hat m = \\frac{m_t}{1-\\beta _1 ^t}$\n",
    "* $\\hat v = \\frac{v_t}{1-\\beta _2 ^t}$\n",
    "* $W_{t+1} = W_t - \\frac{\\eta}{\\sqrt{\\hat v_t} + \\epsilon} \\hat m_t $\n",
    "* 这是非常著名的Adam算法，应用的非常之多。就想Adagrad 和RMSprop会存储之前衰减的平方梯度，同时也会保存衰减的梯度，更新方式和前面的类似。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
