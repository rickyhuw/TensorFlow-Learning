{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer\n",
    "## 1.梯度下降法（Gradient Descent）\n",
    "* 标准的梯度下降法\n",
    "    - 先计算所有样本的总误差，然后根据总误差更新权值\n",
    "* 随机梯度下降法\n",
    "    - 随机抽取一个样本计算误差，然后更新权值\n",
    "* 批量梯度下降法\n",
    "    - 随机抽取一个batch的样本计算误差，然后更新权值\n",
    "* 公式：$ W = W - \\eta \\bullet \\nabla J(W;x^{(i)},y^{(i)})$\n",
    "   \n",
    "## 2.动力下降法（Momentum）\n",
    "* 当前权值的改变会受到上一次改变的影响。比如小球下坡的时候会带有惯性，因此在上一次的梯度较大时，下一次会加速下降。\n",
    "* $\\gamma$:动力，一般取值为0.9\n",
    "* $V_t = \\gamma V_{t-1} + \\eta \\bullet \\nabla J(W)$\n",
    "* $W = W - V_t$\n",
    "\n",
    "## 3.NAG（Nesterov accelerated gradient）\n",
    "* 内斯特罗夫加速梯度\n",
    "* 这个是对“动力下降法”的改良，因为动力下降，在接近谷底的时候，会因为惯性的原因，导致跳出最优解，因此NAG在此基础之上，添加了对小球下一次位置的预测，然后应用到本次的下降中。\n",
    "* $V_t = \\gamma V_{t-1} + \\eta \\bullet \\nabla J(W - \\gamma V_{t-1})$\n",
    "* $W = W - V_t$\n",
    "\n",
    "## 4.Adagrad\n",
    "* $i$:表示第i个分类\n",
    "* $t$:表示某个类别出现的次数\n",
    "* $\\epsilon$:表示一个非常小的值，防止除0\n",
    "* $\\eta$:学习率\n",
    "* $g_{t,i} =  \\nabla _w J(W)$: i类别在第t次的梯度\n",
    "* $W_{t + 1} = W_t - \\frac {\\eta}{\\sqrt{\\sum_{t^\"=1}^{t}{(g_{t^\",i})}^2 + \\epsilon }} \\bullet g_t$\n",
    "* 它是基于SGD思想的一种优化算法，对于比较常见的数据（分类）会有比较小的学习率，对于比较罕见的数据（分类）会有比较大的学习率，使用与比较稀疏的数据集。因此他的优点也恰恰是他的缺点，优点是可以自动的调整学习率；缺点是随着迭代次数的增大，学习率会越来越小，最终趋于0.\n",
    "\n",
    "\n",
    "## 5.RMSprop\n",
    "* 和Adagrad的思想非常相似，唯一的不同是，不在是记录所有的t了，而是取前t次的累加。\n",
    "\n",
    "## 6.Adadelta\n",
    "\n",
    "## 7. Adam\n",
    "* $\\beta _1$:一般取值为0.9\n",
    "* $\\beta _2$:一般取值为0.99\n",
    "* $\\epsilon$:表示一个非常小的值，防止除0\n",
    "* $m_t = \\beta _1 m_{t-1} + (1-\\beta _1)g_t$\n",
    "* $v_t = \\beta _2 v_{t-1} + (1- \\beta _2)g_t^2$\n",
    "* $\\hat m = \\frac{m_t}{1-\\beta _1 ^t}$\n",
    "* $\\hat v = \\frac{v_t}{1-\\beta _2 ^t}$\n",
    "* $W_{t+1} = W_t - \\frac{\\eta}{\\sqrt{\\hat v_t} + \\epsilon} \\hat m_t $\n",
    "* 这是非常著名的Adam算法，应用的非常之多。就想Adagrad 和RMSprop会存储之前衰减的平方梯度，同时也会保存衰减的梯度，更新方式和前面的类似。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "epoch:0, Testing Accuracy:0.9422, Training Accuracy:0.9448364, Learning Rate:0.001\n",
      "epoch:1, Testing Accuracy:0.9552, Training Accuracy:0.9606, Learning Rate:0.00095\n",
      "epoch:2, Testing Accuracy:0.9606, Training Accuracy:0.9680727, Learning Rate:0.0009025\n",
      "epoch:3, Testing Accuracy:0.9622, Training Accuracy:0.9717636, Learning Rate:0.000857375\n",
      "epoch:4, Testing Accuracy:0.9694, Training Accuracy:0.9782364, Learning Rate:0.00081450626\n",
      "epoch:5, Testing Accuracy:0.9707, Training Accuracy:0.9794545, Learning Rate:0.0007737809\n",
      "epoch:6, Testing Accuracy:0.97, Training Accuracy:0.9811636, Learning Rate:0.0007350919\n",
      "epoch:7, Testing Accuracy:0.9684, Training Accuracy:0.98081815, Learning Rate:0.0006983373\n",
      "epoch:8, Testing Accuracy:0.9715, Training Accuracy:0.98507273, Learning Rate:0.0006634204\n",
      "epoch:9, Testing Accuracy:0.9729, Training Accuracy:0.9863818, Learning Rate:0.0006302494\n",
      "epoch:10, Testing Accuracy:0.9753, Training Accuracy:0.98834544, Learning Rate:0.0005987369\n",
      "epoch:11, Testing Accuracy:0.9738, Training Accuracy:0.98754543, Learning Rate:0.0005688001\n",
      "epoch:12, Testing Accuracy:0.9756, Training Accuracy:0.9887818, Learning Rate:0.0005403601\n",
      "epoch:13, Testing Accuracy:0.9755, Training Accuracy:0.9902909, Learning Rate:0.0005133421\n",
      "epoch:14, Testing Accuracy:0.977, Training Accuracy:0.99078184, Learning Rate:0.000487675\n",
      "epoch:15, Testing Accuracy:0.9751, Training Accuracy:0.99054545, Learning Rate:0.00046329122\n",
      "epoch:16, Testing Accuracy:0.9762, Training Accuracy:0.99227273, Learning Rate:0.00044012666\n",
      "epoch:17, Testing Accuracy:0.9776, Training Accuracy:0.993, Learning Rate:0.00041812033\n",
      "epoch:18, Testing Accuracy:0.9782, Training Accuracy:0.99292725, Learning Rate:0.00039721432\n",
      "epoch:19, Testing Accuracy:0.9774, Training Accuracy:0.9932, Learning Rate:0.0003773536\n",
      "epoch:20, Testing Accuracy:0.9743, Training Accuracy:0.9925454, Learning Rate:0.00035848594\n"
     ]
    }
   ],
   "source": [
    "# 手写体识别数据集\n",
    "# http://yann.lecun.com/exdb/mnist/\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data',one_hot=True)\n",
    "\n",
    "# batch\n",
    "batch_size = 100\n",
    "# 批次数\n",
    "n_batch = mnist.train.num_examples // batch_size\n",
    "\n",
    "# learning_rate\n",
    "learning_rate = 0.2\n",
    "\n",
    "# 占位符\n",
    "x = tf.placeholder(tf.float32,[None,784])\n",
    "y = tf.placeholder(tf.float32,[None,10])\n",
    "keep_probility = tf.placeholder(tf.float32)\n",
    "lr = tf.Variable(0.001,dtype=tf.float32)\n",
    "\n",
    "# 创建一个单层的神经网络\n",
    "W1 = tf.Variable(tf.truncated_normal([784,500],stddev=0.1))\n",
    "b1 = tf.Variable(tf.zeros([500]) + 0.1)\n",
    "L1 = tf.tanh(tf.matmul(x,W1) + b1)\n",
    "L1_drop = tf.nn.dropout(L1,keep_prob=keep_probility)\n",
    "\n",
    "W2 = tf.Variable(tf.truncated_normal([500,300],stddev=0.1))\n",
    "b2 = tf.Variable(tf.zeros([300]) + 0.1)\n",
    "L2 = tf.tanh(tf.matmul(L1_drop,W2) + b2)\n",
    "L2_drop = tf.nn.dropout(L2,keep_prob=keep_probility)\n",
    "\n",
    "W3 = tf.Variable(tf.truncated_normal([300,100],stddev=0.1))\n",
    "b3 = tf.Variable(tf.zeros([100]) + 0.1)\n",
    "L3 = tf.tanh(tf.matmul(L2_drop,W3) + b3)\n",
    "L3_drop = tf.nn.dropout(L3,keep_prob=keep_probility)\n",
    "\n",
    "W4 = tf.Variable(tf.truncated_normal([100,10],stddev=0.1))\n",
    "b4 = tf.Variable(tf.zeros([10]) + 0.1)\n",
    "prediction = tf.nn.softmax(tf.matmul(L3_drop,W4) + b4)\n",
    "\n",
    "# loss\n",
    "# loss = tf.reduce_mean(tf.square(y - prediction))\n",
    "\n",
    "# 使用交叉熵代价函数\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=prediction))\n",
    "\n",
    "# train\n",
    "# train_step = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "train_step = tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "# init\n",
    "init = tf.global_variables_initializer()\n",
    "# accuracy\n",
    "correct_predictions = tf.equal(tf.arg_max(y,1),tf.arg_max(prediction,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predictions,tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(21):\n",
    "        sess.run(tf.assign(lr, 0.001 * (0.95 ** epoch)))\n",
    "        for batch in range(n_batch):\n",
    "            batch_xs,batch_ys = mnist.train.next_batch(batch_size)\n",
    "            sess.run(train_step,feed_dict={x:batch_xs,y:batch_ys,keep_probility:0.9})\n",
    "        lr_rate = sess.run(lr)\n",
    "        test_acc = sess.run(accuracy,feed_dict={x:mnist.test.images,y:mnist.test.labels,keep_probility:0.9})\n",
    "        train_acc = sess.run(accuracy,feed_dict={x:mnist.train.images,y:mnist.train.labels,keep_probility:0.9})\n",
    "        print('epoch:'+ str(epoch) + \", Testing Accuracy:\" + str(test_acc) + \", Training Accuracy:\" + str(train_acc) + \", Learning Rate:\" + str(lr_rate)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
